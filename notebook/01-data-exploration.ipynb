{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration - Legal Text Decoder\n",
    "\n",
    "This notebook explores the preprocessed dataset for the Legal Text Decoder project.\n",
    "\n",
    "**Dataset Overview:**\n",
    "- Hungarian legal texts (ÁSZF - Általános Szerződési Feltételek)\n",
    "- Labeled for understandability on a scale of 1-5\n",
    "- Data collected from multiple students using Label Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data\n",
    "with open('../data/processed/all_data.json', 'r', encoding='utf-8') as f:\n",
    "    all_data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(all_data)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"  Columns: {list(df.columns)}\")\n",
    "print(f\"  Total samples: {len(df)}\")\n",
    "print(f\"  Source folders: {df['source_folder'].nunique()}\")\n",
    "print(f\"  Source files: {df['source_file'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Label Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution\n",
    "label_counts = df['label'].value_counts().sort_index()\n",
    "label_names = {\n",
    "    1: '1-Nagyon nehezen érthető',\n",
    "    2: '2-Nehezen érthető',\n",
    "    3: '3-Többé/kevésbé megértem',\n",
    "    4: '4-Érthető',\n",
    "    5: '5-Könnyen érthető'\n",
    "}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "colors = sns.color_palette('RdYlGn', 5)\n",
    "bars = axes[0].bar(label_counts.index, label_counts.values, color=colors)\n",
    "axes[0].set_xlabel('Understandability Label', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Label Distribution', fontsize=14)\n",
    "axes[0].set_xticks([1, 2, 3, 4, 5])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, label_counts.values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10, \n",
    "                 f'{count}\\n({100*count/len(df):.1f}%)', \n",
    "                 ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(label_counts.values, labels=[f'{i}' for i in label_counts.index], \n",
    "            autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "axes[1].set_title('Label Proportions', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebook/label_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLabel counts:\")\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"  {label_names[label]}: {count} ({100*count/len(df):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text length columns\n",
    "df['text_length_chars'] = df['text'].str.len()\n",
    "df['text_length_words'] = df['text'].str.split().str.len()\n",
    "\n",
    "# Text length statistics\n",
    "print(\"Text Length Statistics (characters):\")\n",
    "print(df['text_length_chars'].describe())\n",
    "\n",
    "print(\"\\nText Length Statistics (words):\")\n",
    "print(df['text_length_words'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram of character lengths\n",
    "axes[0].hist(df['text_length_chars'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Text Length (characters)', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Distribution of Text Lengths', fontsize=14)\n",
    "axes[0].axvline(df['text_length_chars'].median(), color='red', linestyle='--', label=f'Median: {df[\"text_length_chars\"].median():.0f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Box plot by label\n",
    "df.boxplot(column='text_length_chars', by='label', ax=axes[1])\n",
    "axes[1].set_xlabel('Understandability Label', fontsize=12)\n",
    "axes[1].set_ylabel('Text Length (characters)', fontsize=12)\n",
    "axes[1].set_title('Text Length by Label', fontsize=14)\n",
    "plt.suptitle('')  # Remove automatic title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebook/text_length_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length by label\n",
    "print(\"\\nMean text length (chars) by label:\")\n",
    "for label in sorted(df['label'].unique()):\n",
    "    mean_len = df[df['label'] == label]['text_length_chars'].mean()\n",
    "    print(f\"  Label {label}: {mean_len:.1f} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Source Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples per source folder\n",
    "folder_counts = df['source_folder'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "bars = plt.bar(range(len(folder_counts)), folder_counts.values)\n",
    "plt.xticks(range(len(folder_counts)), folder_counts.index, rotation=45, ha='right')\n",
    "plt.xlabel('Source Folder (Student ID)', fontsize=12)\n",
    "plt.ylabel('Number of Samples', fontsize=12)\n",
    "plt.title('Samples per Data Source', fontsize=14)\n",
    "\n",
    "# Add count labels\n",
    "for i, (bar, count) in enumerate(zip(bars, folder_counts.values)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n",
    "             str(count), ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebook/samples_per_source.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal source folders: {len(folder_counts)}\")\n",
    "print(f\"Samples per folder: min={folder_counts.min()}, max={folder_counts.max()}, mean={folder_counts.mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution by source (heatmap)\n",
    "source_label_dist = pd.crosstab(df['source_folder'], df['label'], normalize='index') * 100\n",
    "\n",
    "plt.figure(figsize=(10, 12))\n",
    "sns.heatmap(source_label_dist, annot=True, fmt='.0f', cmap='RdYlGn', \n",
    "            cbar_kws={'label': 'Percentage'})\n",
    "plt.xlabel('Understandability Label', fontsize=12)\n",
    "plt.ylabel('Source Folder', fontsize=12)\n",
    "plt.title('Label Distribution (%) by Data Source', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebook/label_by_source_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples from each label\n",
    "print(\"Sample texts for each understandability level:\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for label in sorted(df['label'].unique()):\n",
    "    sample = df[df['label'] == label].sample(1).iloc[0]\n",
    "    print(f\"\\n[LABEL {label}: {label_names[label]}]\")\n",
    "    print(\"-\"*80)\n",
    "    text = sample['text']\n",
    "    # Truncate long texts\n",
    "    if len(text) > 500:\n",
    "        text = text[:500] + \"...\"\n",
    "    print(text)\n",
    "    print(f\"\\nSource: {sample['source_folder']}/{sample['source_file']}\")\n",
    "    print(f\"Length: {len(sample['text'])} chars\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train/Val/Test Split Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load splits\n",
    "splits = {}\n",
    "for split_name in ['train', 'val', 'test']:\n",
    "    with open(f'../data/processed/{split_name}.json', 'r', encoding='utf-8') as f:\n",
    "        splits[split_name] = pd.DataFrame(json.load(f))\n",
    "\n",
    "# Verify split sizes\n",
    "print(\"Split sizes:\")\n",
    "for name, split_df in splits.items():\n",
    "    print(f\"  {name}: {len(split_df)} ({100*len(split_df)/len(df):.1f}%)\")\n",
    "\n",
    "# Check for data leakage (no overlap between splits)\n",
    "train_texts = set(splits['train']['text'])\n",
    "val_texts = set(splits['val']['text'])\n",
    "test_texts = set(splits['test']['text'])\n",
    "\n",
    "train_val_overlap = len(train_texts.intersection(val_texts))\n",
    "train_test_overlap = len(train_texts.intersection(test_texts))\n",
    "val_test_overlap = len(val_texts.intersection(test_texts))\n",
    "\n",
    "print(f\"\\nData leakage check:\")\n",
    "print(f\"  Train-Val overlap: {train_val_overlap}\")\n",
    "print(f\"  Train-Test overlap: {train_test_overlap}\")\n",
    "print(f\"  Val-Test overlap: {val_test_overlap}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution across splits\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, (name, split_df) in enumerate(splits.items()):\n",
    "    counts = split_df['label'].value_counts().sort_index()\n",
    "    colors = sns.color_palette('RdYlGn', 5)\n",
    "    axes[i].bar(counts.index, counts.values, color=colors)\n",
    "    axes[i].set_xlabel('Label', fontsize=12)\n",
    "    axes[i].set_ylabel('Count', fontsize=12)\n",
    "    axes[i].set_title(f'{name.capitalize()} Set (n={len(split_df)})', fontsize=14)\n",
    "    axes[i].set_xticks([1, 2, 3, 4, 5])\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for j, (label, count) in enumerate(counts.items()):\n",
    "        axes[i].text(label, count + 1, f'{100*count/len(split_df):.1f}%', \n",
    "                     ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../notebook/split_distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "summary = {\n",
    "    'Metric': [\n",
    "        'Total Samples',\n",
    "        'Training Samples',\n",
    "        'Validation Samples',\n",
    "        'Test Samples',\n",
    "        'Number of Labels',\n",
    "        'Source Folders',\n",
    "        'Min Text Length (chars)',\n",
    "        'Max Text Length (chars)',\n",
    "        'Mean Text Length (chars)',\n",
    "        'Median Text Length (chars)',\n",
    "    ],\n",
    "    'Value': [\n",
    "        len(df),\n",
    "        len(splits['train']),\n",
    "        len(splits['val']),\n",
    "        len(splits['test']),\n",
    "        df['label'].nunique(),\n",
    "        df['source_folder'].nunique(),\n",
    "        df['text_length_chars'].min(),\n",
    "        df['text_length_chars'].max(),\n",
    "        f\"{df['text_length_chars'].mean():.1f}\",\n",
    "        f\"{df['text_length_chars'].median():.0f}\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(\"\\nDataset Summary:\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
